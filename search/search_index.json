{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#some-general-reading","title":"Some general reading ...","text":"<p>Question</p> <p>What is data science?</p> <p>Data science is an interdisciplinary field that involves the extraction of insights from data using statistical, computational, and machine learning techniques. Data science involves the entire process of collecting, processing, analyzing, and interpreting data to solve complex problems and make data-driven decisions.</p> <p>Question</p> <p>Why is data science important?</p> <p>Data science is important because it allows us to make sense of vast amounts of data that are generated every day. By extracting insights from data, we can identify patterns, trends, and relationships that can inform business decisions, scientific research, and public policy. Data science also plays a critical role in developing and improving machine learning algorithms that power many modern technologies.</p> <p>Question</p> <p>What are some common applications of data science?</p> <p>Data science is used in a wide range of industries and fields, including business, healthcare, finance, marketing, and scientific research. Some common applications of data science include fraud detection, customer segmentation, predicting disease outbreaks, personalized recommendations, and image and speech recognition.</p> <p>Question</p> <p>What are data science tools?</p> <p>Data science tools are software applications, libraries, and frameworks that are used to facilitate the various tasks involved in data science, such as data manipulation, data analysis, machine learning, and data visualization.</p>"},{"location":"#session-1","title":"Session 1","text":"<pre><code>    %%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'showBranches': true, 'showCommitLabel':true,'mainBranchName': 'session1'}} }%%\n\n    gitGraph\n       checkout session1\n       commit id: \"Git &amp; Version control\"\n       commit id: \"Python package management\"\n       branch foo\n       checkout foo\n       commit id: \"Anaconda + Conda\"\n       commit id: \"pip\"\n       commit id: \"poetry\"\n       checkout session1\n       merge foo\n       commit id: \"Notebooks\"\n</code></pre>"},{"location":"#session-2","title":"Session 2","text":"<pre><code>    %%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'showBranches': true, 'showCommitLabel':true,'mainBranchName': 'session2'}} }%%\n\n    gitGraph\n       commit id: \"1\"\n       branch docker\n       checkout docker\n       commit id: \"postgres\"\n       commit id: \"pgadmin\"\n       commit id: \"metabase\"\n       checkout session2\n       merge docker\n       commit id: \"2\"\n       branch docker-compose\n       checkout docker-compose\n       commit id: \"all at once\"\n       checkout session2\n       merge docker-compose\n\n\n</code></pre>"},{"location":"#session-3","title":"Session 3","text":"<p>This session will be oriented around airflow.</p>"},{"location":"airflow/","title":"Airflow","text":"<p>Airflow is an open-source platform initially develop by Airbnb to schedule, and orchestrate  of complex data workflows. It is primarily used in the field of data engineering and data processing to manage the  movement and transformation of data between systems and processes.</p> <p>At its core, Airflow provides a workflow automation framework that allows users to define, execute,  and monitor data pipelines as directed acyclic graphs (DAGs).</p> <p>Info</p> <p>You can also use some alternative tool, even if airflow is used in many companies, some of them chose to use  similar tool : </p> <ul> <li>Prefect</li> <li>Luigi</li> <li>Dagster</li> <li>...</li> </ul>"},{"location":"airflow/#whats-a-dag","title":"What's a dag ?","text":"<p>DAGs are graphs that represent the dependencies and  relationships between tasks, where tasks are units of work that perform specific actions on data,  such as data extraction, transformation, or loading. DAGs in Airflow are defined using Python.</p> <p></p>"},{"location":"airflow/#why-do-i-need-such-a-tool","title":"Why do I need such a tool ?","text":"<p>In the data-world, and especially when you're building an analytics stack you'll perform three main types of operations * Extract data from sources : it can be API, Databases, files, using scraping technics etc... * Transform the data : you'll probably clean it, perform some aggregation, consolidate it with other sources * Load the data : you'll put the data in an appropriate system depending on your needs, for example in a warehouse   and plug a bi tool on top of it. You will also use it to expose the data to other systems / applications (ML etc...)</p> <p>To orchestrate all those operations and implement the logic between them, you'll need some tool : airflow is one of them.</p> <p>You will meet two main architectural pattern : ETL &amp; ELT. Even if they look very similar, they are not and they have their own pros and cons</p>"},{"location":"airflow/#recap-data-integration-pattern","title":"Recap : Data integration pattern","text":""},{"location":"airflow/#etl","title":"ETL","text":"<p>ETL is the more \"traditional\" approach, where you prepare your data before loading it to your target system. It means that the workflow is performed by the transformation engine.</p> <p></p>"},{"location":"airflow/#elt","title":"ELT","text":"<p>ELT is the more modern approach, the main point here is that you load the data first, and process the data after. In this case, most the computation is done on the target system directly, which act as the transformation  engine. This is a common approach used when your core component is a warehouse, like snowflake of bigquery, designed to handle intensive computation</p> <p></p>"},{"location":"airflow/#how-does-it-work","title":"How does it work ?!","text":"<p>Tip</p> <p>It's not mandatory to be able to explain how airflow works to use it, however it's a good thing to have a good  understanding of the tool you're using...</p> <p>As a user, you will interact with 3 components : </p> <ul> <li>You will configure airflow : writing the <code>airflow.cfg</code> file</li> <li>You will use the UI :  for managing and monitoring workflows. It allows you to interact with Airflow,  such as viewing DAGs, triggering DAG runs, monitoring task statuses, and managing connections, variables etc...  </li> <li>You will write python DAGs</li> </ul> <p>But there are not the only required components... Here is a brief explanation of the role of each component :</p> <ul> <li> <p>Worker: A worker is a component in the Airflow architecture that is responsible for executing tasks.  When a task is triggered for execution, it is assigned to a worker, which runs the task and reports the result back  to the scheduler. Workers can be horizontally scaled to handle a large number of tasks concurrently,  making them suitable for parallel processing of tasks.</p> </li> <li> <p>Scheduler The scheduler is the core component of Airflow that manages the scheduling and execution of tasks.  It determines when tasks should be executed based on their dependencies, triggers, and time-based schedules defined in  the DAGs (directed acyclic graphs). The scheduler communicates with the metadata database to keep track of task status,  schedule changes, and other metadata. It also communicates with workers to assign tasks for execution and monitor their  progress.</p> </li> </ul> <p></p> <p>Metadata Database: Airflow uses a SQL database to store metadata about the data pipelines being run. In the diagram above, this is represented as Postgres which is extremely popular with Airflow.</p> <p>Extract from airflow documentation</p>"},{"location":"airflow/#how-to-deploy-airflow","title":"How to deploy airflow ?","text":"<p>There are several way to deploy airflow, the choice will highly depends on your requirements !</p> <p>Warning</p> <p>Be careful, some deployments are not suitable for production</p>"},{"location":"airflow/#deploy-airflow-locally","title":"Deploy airflow locally","text":"<p>Here the link to the airflow documentation explaining how to easily deploy it on your local machine.</p> <p>Info</p> <p>You will probably use it for development !</p>"},{"location":"airflow/#using-a-cloud-managed-service","title":"Using a cloud managed service","text":"<p>Depending on the provider you're working on, you'll use different service with their specificities. One of the biggest difference will be the way of providing your dag to the service.</p>"},{"location":"airflow/#on-aws","title":"On AWS","text":"<p>Aws has developed a service called MWAA built on top of airflow, it allows you to not taking care of the management  of the underlying infrastructure... which can be great when you don't have the resources / knowledges !</p> <p>The only major difference for a developer is the way of providing your dags, with MWAA you'll need to push them to S3 </p> <p></p> <p>Extract from aws documentation</p>"},{"location":"airflow/#on-gcp","title":"On GCP","text":"<p>Google has developed Cloud composer, also built on top of airflow.</p>"},{"location":"airflow/#deploy-airflow-on-kubernetes-on-premise-or-in-a-cloud-environment","title":"Deploy airflow on kubernetes (on-premise or in a cloud environment)","text":"<p>Info</p> <p>You can manage a kubernetes cluster ... or use the managed service of your cloud provider.</p> <p>Airflow can be deployed on Kubernetes, a popular container orchestration platform, which provides advanced capabilities for managing containerized applications at scale. Deploying Airflow on Kubernetes allows for dynamic scaling,  rolling updates, and self-healing of Airflow components, making it suitable for large-scale and production-grade  workflows.</p>"},{"location":"airflow/#how-to-create-a-dag","title":"How to create a dag","text":"<p>As previously mentioned, a dag can be written in python... and it's simple !</p>"},{"location":"airflow/#airflow-operators","title":"Airflow operators","text":"<p>We've seen that a dag is an association of tasks, with dependencies, but what is a task ? A task can be the execution of a python function, some shell commands, an SQL request etc...</p> <p>The great thing here is that airflow come with some pre-built <code>Operators</code> that you can use in your tasks.</p> <ul> <li><code>BashOperator</code> - executes a bash command</li> <li><code>PythonOperator</code> - calls a Python function</li> <li><code>PostgresOperator</code> - To execute an SQL request on a postgres database</li> <li><code>MySqlOperator</code> - To execute an SQL request on a mysql database</li> <li>... A lot of operator exists, and allows you to interact with external system easily</li> </ul> <p></p>"},{"location":"airflow/#example-a-first-minimal-dag","title":"Example - A first minimal dag","text":"<pre><code>from datetime import datetime\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\ndef print_hello_world():\nreturn 'Hello world !'\nwith DAG('hello_world',\ndescription='Hello world dag',\nschedule_interval='0 1 * * *',\nstart_date=datetime(2023, 4, 10)\n) as dag:\nprint_hello_operator = PythonOperator(task_id='hello_task', python_callable=print_hello_world, dag=dag)\nprint_hello_operator\n</code></pre> <p>Info</p> <p>We can now visualize our dag on the airflow UI. By default, any dag is not activated, we need to switch the left  toggle to activate it, and allow the scheduler to trigger it.</p> <p></p> <p>We've defined, a dag containing a single task, it's not really useful in the real world. Let's create an other examples with multiple tasks.</p>"},{"location":"airflow/#example","title":"Example","text":"<p>Assuming we want to create a daily job that move some files from one source directory to a target directory : </p> <ul> <li>Let's define a function that list the files in the source</li> <li>Implement another function that task as parameter the files from task1 and move the files from the source to the destination</li> </ul> <pre><code>import os\nimport shutil\nfrom datetime import datetime\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\ndef get_files_in_directory(directory_path: str) -&gt; list[str]:\nreturn os.listdir(directory_path)\ndef move_files(files_name: list[str], source_directory: str, target_directory: str) -&gt; None:\nfor file_name in files_name:\n# Construct source and target file paths\nsource_file = os.path.join(source_directory, file_name)\ntarget_file = os.path.join(target_directory, file_name)\n# Move the file from source to target\nshutil.move(source_file, target_file)\nwith DAG('move_file_dag', schedule_interval='0 1 * * *', start_date=datetime(2023, 4, 13)) as dag:\nget_files_task = PythonOperator(\ntask_id='get_files_task',\nop_args=[\"./sources\"],\npython_callable=get_files_in_directory,\ndag=dag\n)\nmove_files_task = PythonOperator(\ntask_id='move_files_task',\npython_callable=move_files,\nop_args=[get_files_task.output, \"./sources\", \"./destination\"],  # Pass output of get_files_task as argument\ndag=dag\n)\nget_files_task &gt;&gt; move_files_task\n</code></pre> <p>Info</p> <p>As you can see, to define the relationship between the two task, we use <code>&gt;&gt;</code> operator, you can also wrap multiple tasks between square brackets to be dependent on the same upstream task. For example :  <code>first_task &gt;&gt; second_task &gt;&gt; [third_task, fourth_task]</code></p>"},{"location":"airflow/#successful-dag-execution","title":"Successful dag execution","text":"<p>In case of success, you should have all your tasks in green :</p> <p></p>"},{"location":"airflow/#failing-dag-execution","title":"Failing dag execution","text":"<p>If the first task <code>get_files_task</code> fails, the second task <code>move_files_task</code> will not be executed (with the default config)  Here a visual example of this case : </p> <p></p>"},{"location":"airflow/#some-interesting-features","title":"Some interesting features","text":"<p>We've seen some very basic features of airflow, it has a lot of other features, that can help you to build some more complex pipeline.</p>"},{"location":"airflow/#callback","title":"Callback","text":"<p>A <code>Callback</code> is a function that is only invoked when the task state changes due to execution by a worker. For example, when a task fails. </p> <p>Tip</p> <p>One good example of callback use is to send some alert when a task fail, it's very common to trigger an alert sent by slack or by email to inform that a task has fail.</p> <p>There are fours type of callback : </p> Name Description on_success_callback Invoked when the task succeeds on_failure_callback Invoked when the task fails sla_miss_callback Invoked when a task misses its defined SLA on_retry_callback Invoked when the task is up for retry <p>From airflow documentation</p>"},{"location":"airflow/#connections","title":"Connections","text":"<p>Connections are used for storing credentials that are used to connect to external systems such as databases.</p> <p>A Connection is essentially set of parameters - such as username, password and hostname -  along with the type of system that it connects to, and a unique name, called the conn_id.</p> <p>They can be managed via the UI or via the CLI.</p> <p>Via the UI, you'll need to go to <code>Admin&gt;&gt;Connections</code> and will have the following page:</p> <p></p> <p>You'll need to install some additional dependencies to use some pre-built connections. For postgresql, you need to  install the package : <code>pip install apache-airflow-providers-postgres</code> </p> <p>Warning</p> <p>After installing a dependency, you'll need to restart your airflow instance</p>"},{"location":"airflow/#hooks","title":"Hooks","text":"<p>A hook is an interface that allow you to easily communicate with an external system (database for example), they use the connections to connect to those system. As they encapsulate the details of connecting to and interacting  with external systems, it helps you to perform some operations with a minimal amount of code.</p> <p>Tip</p> <p>If you need to interact with a Postgres database, have a look to the <code>PostgresHook</code></p>"},{"location":"airflow/#cron-format-reminder","title":"Cron format reminder","text":"<p>If you have any doubt on your schedule format, check it on cron-guru</p> <p>Cron format</p> <pre><code>* * * * *\n- - - - -\n| | | | |\n| | | | ----- Day of week (0 - 7) (Sunday is both 0 and 7)\n| | | ------- Month (1 - 12)\n| | --------- Day of month (1 - 31)\n| ----------- Hour (0 - 23)\n------------- Minute (0 - 59)\n</code></pre>"},{"location":"airflow/#example_1","title":"Example","text":"<ul> <li>To run a command every day at 3:30 AM: <code>30 3 * * *</code></li> <li>To run a command every week on Sunday at 8:00 PM: <code>0 20 * * 0</code></li> <li>To run a command every month on the 15th at 1:45 PM: <code>45 13 15 * *</code></li> </ul>"},{"location":"docker/","title":"Docker","text":""},{"location":"docker/#introduction","title":"Introduction","text":"<p>Docker is a software platform that allows developers to package, distribute, and run applications in a containerized  environment. Containers are lightweight, portable, and self-contained environments that can run almost anywhere,  from a developer's laptop to a cloud-based infrastructure.</p>"},{"location":"docker/#what-is-containerization","title":"What is containerization ?","text":""},{"location":"docker/#key-concepts-components","title":"Key concepts &amp; components","text":"<p>Tip</p> <p>The best way to be familiar with docker is to practice, also they provide a great documentation</p>"},{"location":"docker/#1-dockerfile","title":"1. Dockerfile","text":"<p>A Dockerfile is a text file that contains instructions on how to build a Docker image. The Docker image is a binary file  that contains everything needed to run an application, including the code, libraries, and dependencies. </p>"},{"location":"docker/#2-docker-image","title":"2. Docker image","text":"<p>A Docker image is a snapshot of a container, which includes the application code, libraries, and dependencies.  Images can be built from a Dockerfile or pulled from a public or private Docker registry.</p>"},{"location":"docker/#3-docker-registry","title":"3. Docker registry","text":"<p>A Docker registry is a repository for storing and sharing Docker images. Docker Hub is the most popular public Docker  registry, but you can also use private registries for your organization's images.</p>"},{"location":"docker/#4-docker-container","title":"4. Docker container","text":"<p>A Docker container is a lightweight, standalone executable package that includes everything needed to run an  application, including the application code, libraries, and dependencies. Containers are isolated from the host  system and from other containers, making them a secure way to run applications.</p>"},{"location":"docker/#5-docker-compose","title":"5. Docker Compose","text":"<p>Docker Compose is a tool for defining and running multi-container Docker applications. With Docker Compose, you can define the services that make up your application, their configuration, and the network they should use to communicate with each other.</p>"},{"location":"docker/#6-docker-swarm","title":"6. Docker Swarm","text":"<p>Info</p> <p>It's not very common to use it in a production environment, the standard is to use kubernetes</p> <p>Docker Swarm is a native clustering and orchestration tool for Docker. With Swarm, you can create and manage a cluster  of Docker nodes, and deploy and manage Docker services across the cluster.</p>"},{"location":"docker/#commands-recap","title":"Commands recap","text":"<p>Tip</p> <p>Once again, you don't need to remind all the commands, just practice and go through the documentation ! </p> Command Description docker run Run a container docker ps List running containers docker images List available images docker build Build an image from a Dockerfile docker push Push an image to a remote registry docker pull Pull an image from a remote registry docker stop Stop a running container docker rm Remove a container docker rmi Remove an image docker exec Execute a command in a running container docker logs View the logs of a container"},{"location":"git/","title":"Git","text":""},{"location":"git/#what-we-have-seen-last-week","title":"What we have seen last week","text":"<ul> <li>git concept &amp; basic commands</li> <li>branches</li> <li>pull request &amp; review process</li> <li>simple feature branch workflow</li> <li>ci/cd introduction</li> <li>zoom on github</li> </ul>"},{"location":"git/#commands-recap","title":"Commands recap","text":"<p>Tip</p> <p>You don't need to remind all the commands, just practice and go through the documentation or man page !</p> Command Description <code>git init</code> Initialize a git repository <code>git add &lt;file_name&gt;</code> Add a file to staging area <code>git commit -m \"an explicit message\"</code> Commit changes with an explicit message <code>git push origin &lt;branch_name&gt;</code> Push changes to a remote branch <code>git branch &lt;branch_name&gt;</code> Create a new branch <code>git checkout &lt;branch_name&gt;</code> Switch to a branch <code>git checkout -b &lt;branch_name&gt;</code> Create and switch to a new branch"},{"location":"git/#branch-coding-workflow","title":"Branch &amp; coding workflow","text":"<p>Danger</p> <p>Reminder: Don't code on master branch</p>"},{"location":"git/#the-minimal-workflow-can-be-used-when-coding-alone","title":"The minimal workflow, can be used when coding alone","text":"<pre><code>---\ntitle: Minimal workflow\n---\n%%{init: { 'logLevel': 'debug', 'theme': 'base' } }%%\ngitGraph\n   commit\n   commit\n   branch develop\n   checkout develop\n   commit\n   commit\n   checkout main\n   merge develop\n   commit\ncommit</code></pre>"},{"location":"git/#a-basic-feature-branch-workflow","title":"A basic feature branch workflow","text":"<pre><code>---\ntitle: Basic feature branch workflow\n---\n    %%{init: { 'logLevel': 'debug', 'theme': 'base' } }%%\n    gitGraph\n       commit id: \"1\"\n       commit id: \"2\"\n       branch feature_1\n       checkout feature_1\n       commit id: \"3\"\n       checkout main\n       commit id: \"4\"\n       checkout main\n       branch feature_2\n       checkout feature_2\n       commit id: \"5\"\n       checkout main\n       commit id: \"6\"\n       checkout feature_1\n       commit id: \"7\"\n       checkout main\n       merge feature_1 id: \"customID\" tag: \"customTag\" type: REVERSE\n       checkout feature_2\n       commit id: \"8\"\n       checkout main\n       commit id: \"9\"</code></pre>"},{"location":"git/#best-practices","title":"Best practices","text":""},{"location":"git/#make-single-purpose-small-commits","title":"Make single-purpose &amp; small commits","text":"<p>By creating small commits, it helps everyone in a team to understand what have been done. Also, it's easier to revert a small change in case of bug.</p>"},{"location":"git/#share-only-what-is-necessary-add-gitignore-to-you-repository","title":"Share only what is necessary, add .gitignore to you repository","text":"<p>Info</p> <p><code>.gitignore</code> list all the files and folder that must not be tracked</p> <p>Example</p>"},{"location":"git/#ginignore-simple-example","title":"<code>.ginignore</code> simple example :","text":"<pre><code>__pycache__/\nvenv/\ndata/\ndownload/\nlog.txt\nany_file_you_want_to_exclude.any_extension\n</code></pre>"},{"location":"git/#commit-often-branch-frequently","title":"Commit often &amp; branch frequently","text":"<p>Prefer short-term branch, this will improve the traceability and highly simplify the code review process. Try to not include large number of change in the same branch, and avoid unrelated changes.</p> <p>Tip</p> <p>It's better to commit something un-perfect than nothing  </p>"},{"location":"git/#write-detailed-commit-message-but-short","title":"Write detailed commit message (but short !)","text":"<p>When reading a commit message, anyone should be able to understand what have been done. In general, try to explain what changed from previous code and why.</p> <p>Some good examples: </p> <ul> <li>Change number of epochs from 20 to 40</li> <li>Filter samples that contains null values</li> <li>Change unit from miles to kilometers in compute_distance method</li> </ul> <p>Some bad examples:</p> <ul> <li>Update file1, file2</li> <li>A modification</li> </ul>"},{"location":"python_package_management/","title":"Python package management","text":""},{"location":"python_package_management/#introduction","title":"Introduction","text":"<p>Pip, Anaconda, and Poetry are all package managers for Python, but they have different features and use cases.</p> <p>Info</p> <p>For local development, you can choose between pip &amp; conda depending on your preferences.  In a company, you'll use the one used by the other developers</p>"},{"location":"python_package_management/#pip","title":"Pip","text":"<p>Pip is the default package manager for Python and is used to install and manage Python packages from the Python Package Index (PyPI). Pip is included with Python, and you can use it to install packages globally or in a virtual environment. Pip is simple to use and is suitable for most Python projects.</p>"},{"location":"python_package_management/#anaconda","title":"Anaconda","text":"<p>Anaconda is a distribution of Python that includes many scientific computing packages and tools, such as NumPy, SciPy, and Jupyter notebooks. Anaconda comes with its package manager called conda, which is similar to pip but also includes features for managing non-Python packages and managing virtual environments. Anaconda is particularly useful for data science and machine learning projects because it includes many packages commonly used in those fields.</p>"},{"location":"python_package_management/#poetry","title":"Poetry","text":"<p>Poetry is a newer package manager for Python that aims to simplify dependency management and package distribution. Poetry includes features for managing virtual environments, specifying dependencies and versions, and packaging your project as a distributable package. Poetry is designed to work well with modern Python projects that use tools like Pytest and Black.</p> <p>In summary, if you're working on a simple Python project, pip should suffice. If you're working on a data science or machine learning project, Anaconda might be a better choice. If you're working on a modern Python project that requires advanced dependency management and package distribution, Poetry is worth considering. Ultimately, the choice of package manager depends on your project's requirements and your personal preference.</p>"},{"location":"python_package_management/#commands-recap","title":"Commands recap","text":""},{"location":"python_package_management/#virtual-environment-creation","title":"Virtual environment creation","text":"<p>Warning</p> <ul> <li>You must use virtual env to manage your python projects, basically it will allow you : </li> <li>to prevent version conflicts</li> <li>to create reproducible and easy to install project</li> </ul> <p>There are many way to create virtual env depending on your setup, one you can use : <code>python3 -m venv env_name</code></p>"},{"location":"python_package_management/#pip-usage","title":"Pip usage","text":"<ul> <li>Activate a virtual environment : <code>conda activate env_name</code></li> <li>Install a package : <code>pip install package_name</code></li> <li>Install packages from requirements file : <code>pip install -r requirements.txt</code></li> </ul>"},{"location":"python_package_management/#conda-usage","title":"Conda usage","text":"<ul> <li>Activate a virtual environment : <code>conda activate env_name</code></li> <li>Install a package : <code>conda install package_name</code></li> <li>Install packages from requirements file : <code>conda install --file requirements.txt</code></li> </ul>"},{"location":"practice/airflow/","title":"Airflow in practice","text":""},{"location":"practice/airflow/#installation","title":"Installation","text":"<p>We've seen many way of deploying airflow, for practice, the most straightforward way is to install it via pip and run it in standalone mode.</p> <p>Here a quick tutorial on how to install &amp; run it : airflow documentation</p>"},{"location":"practice/airflow/#create-an-admin-user-to-connect-to-the-ui","title":"Create an admin user to connect to the ui","text":"<p>If you not run all the commands separately, you'll probably need to create a user to connect to the UI</p> <pre><code>airflow users create \\\n--username antoine \\\n--firstname admin \\\n--lastname admin \\\n--role Admin \\\n--password admin \\\n--email admin@example.org\n</code></pre>"},{"location":"practice/airflow/#set-dags-folder","title":"Set dags folder","text":"<p>Tip</p> <p>This is not mandatory, but you can modify the default path of the dag folder to a more convenient place. This is the folder from where airflow will grab the dags you've developed</p> <p>In <code>airflow.cfg</code> file, update <code>dags_folder</code> path to the folder you want on your machine. By default, it's set to your HOME directory.</p>"},{"location":"practice/airflow/#test-to-access-the-ui","title":"Test to access the UI","text":"<p>By default airflow run on localhost:8080. Normally you should have some examples dags that come with the installation if you haven't updated the config file.</p>"},{"location":"practice/airflow/#exercice","title":"Exercice","text":"<p>The aim of the exercise is to create a more realistic dag to retrieve some data from an API, perform some transformation, and store it in a database.</p> <p>Requirements : For this exercise, you'll need : </p> <ul> <li>airflow</li> <li>a database : postgres (feel free to try another)</li> <li>metabase</li> </ul>"},{"location":"practice/airflow/#your-first-dag","title":"Your first dag","text":"<p>Create a hello world dag that run every minute and just print <code>hello world</code>, just to ensure that all is working  as expected.</p>"},{"location":"practice/airflow/#a-real-dag","title":"A real dag","text":"<p>For this exercise, we will use the velib API,</p> <p>Your dag must include (at least the following task):</p> <ul> <li>A task to retrieve the data from the API</li> <li>A task to quickly transform the data (process the data to make it fit the target system)</li> <li>A task to store the result in your a table of your database</li> <li>A task to run some aggregation on the table previously feeded with new data.</li> </ul>"},{"location":"practice/airflow/#1-data-acquisition-task","title":"1 - Data acquisition task","text":"<p>Use the following endpoint : https://velib-metropole-opendata.smoove.pro/opendata/Velib_Metropole/station_status.json to extract the last updated data from the velib api.</p>"},{"location":"practice/airflow/#2-transform-the-data","title":"2 - Transform the data","text":"<p>In this task, you need the json data to be in a flattened format. For each station we want to have the following  information : </p> <ul> <li>station_id</li> <li>number_of_available_docks</li> <li>number_of_electric_bike</li> <li>number_of_mechanical_bike</li> <li>the timestamp at which data has been updated on the server side</li> </ul> <p>Tip</p> <p>You can use the method of your choice, you can do it using pure python, or using some libraries</p>"},{"location":"practice/airflow/#3-store-the-data","title":"3 - Store the data","text":"<pre><code>!!! warning\n\nBefore developing the task, you'll need to create a connection to your postgres database\n</code></pre> <p>Create a task that takes the previously transform data an insert in a table called <code>velib_station_status</code>. You can manage the table creation from your dag (using <code>CREATE TABLE IF NOT EXISTS</code>, (before the insert statement), or do it using the pg cli.</p> <p>Here is the expected schema of the table : </p> Column Name Description Type id ID of the record - primary key INT station_id ID of the station INT number_of_available_docks Number of available docks at the station INT number_of_electric_bike Number of available electric bikes at the station INT number_of_mechanical_bike Number of available mechanical bikes at the station INT timestamp_data_updated_on_server Timestamp at which data has been updated on the server side TIMESTAMP <p>Here is an example form the airflow documentation</p>"},{"location":"practice/airflow/#4-aggregate-the-data","title":"4 - Aggregate the data","text":"<p>Create a last task which execute a postgres request to count : </p> <ul> <li>the total number of free docks</li> <li>the total number of available mechanical bike</li> <li>the total number of available electric bike</li> </ul> <p>The result must be store in another table : <code>stations_aggregates</code> with the following columns :  Here is the expected schema of the table :</p> Column Name Description Type id ID of the record - Primary key INT number_free_docks Number of free docks at the station INT number_available_e_bikes Number of available electric bikes at the station INT number_available_mechanical_bikes Number of available mechanical bikes at the station INT created_at Timestamp of the computation TIMESTAMP <p>Tip</p>"},{"location":"practice/airflow/#5-plug-metabase-on-your-database","title":"5 - Plug metabase on your database","text":""},{"location":"practice/airflow/#bonus","title":"Bonus","text":"<p>Create a slack integration and add an on_failure callback to send an alert when your dag fail</p>"},{"location":"practice/docker/","title":"Docker","text":""},{"location":"practice/docker/#docker-in-practice","title":"Docker in practice","text":"<p>Let's setup a local environment with the following components : </p> <ul> <li>A database : postgresql</li> <li>An database administration platform : pgadmin</li> <li>A Business intelligence app : metabase</li> </ul> <p>Danger</p> <p>This is a local setup, in a real life project, you'll not host your database in a docker container Also, metabase use a database to store its metadata, by default it comes with an sqllite, which must be replaced by another external database like postgres or mysql</p>"},{"location":"practice/docker/#docker-a-simple-way-to-setup-your-local-environment","title":"Docker, a simple way to setup your local environment","text":"<p>You can deploy any application using a very short command... with docker !</p>"},{"location":"practice/docker/#deploy-a-database","title":"Deploy a database","text":"<pre><code>docker run -d \\\n--name postgres \\\n-p 5432:5432 \\\n-e POSTGRES_PASSWORD=password \\\n-v postgres:/var/lib/postgresql/data \\\npostgres:15.2\n</code></pre> <p>Let's decompose this command : </p> Argument Description <code>docker run</code> Command to start a new container <code>-d</code> Runs the container in the background (detached mode) <code>--name postgres</code> Assigns the name <code>postgres</code> to the new container <code>-p 5432:5432</code> Maps port 5432 on the host machine to port 5432 in the container <code>-e POSTGRES_PASSWORD=password</code> Sets an environment variable <code>POSTGRES_PASSWORD</code> with the value <code>password</code> inside the container <code>-v postgres:/var/lib/postgresql/data</code> Mounts a Docker volume named <code>postgres</code> to the container directory <code>/var/lib/postgresql/data</code> <code>postgres:15.2</code> Specifies the name and tag of the PostgreSQL Docker image to use for the new container <p>Now you've a running postgres instance which is listening on port 5432, you can connect to it using <code>psql</code> or any client.</p> <p>You can check it by running <code>docker ps</code> or directly in docker desktop if you prefer the UI.</p> <p>You will have an output like this :  <pre><code>CONTAINER ID   IMAGE           COMMAND                  CREATED         STATUS         PORTS                    NAMES\n3ca9e24601b5   postgres:15.2   \"docker-entrypoint.s\u2026\"   5 seconds ago   Up 4 seconds   0.0.0.0:5432-&gt;5432/tcp   postgres\n</code></pre></p> <p>Tip</p> <p>It's better to use the cli (the commands) instead of the UI, it will help you to become more familiar with docker</p> <p></p>"},{"location":"practice/docker/#deploy-pgadmin","title":"Deploy pgadmin","text":"<p>For the exercise, let's say you want setup a web client to monitor and administrate your database. Let's deploy pg admin !</p> <pre><code>docker run \\\n-p 5050:80 \\\n-e \"PGADMIN_DEFAULT_EMAIL=email@example.com\" \\\n-e \"PGADMIN_DEFAULT_PASSWORD=password\" \\\n-d dpage/pgadmin4:latest\n</code></pre> <p>To be sure you understand... let's describe the command again</p> Argument Description <code>docker run</code> Command to start a new container <code>-p 5050:80</code> Maps port 80 inside the container to port 5050 on the host machine <code>-e \"PGADMIN_DEFAULT_EMAIL=email@example.com\"</code> Sets an environment variable <code>PGADMIN_DEFAULT_EMAIL</code> with the value <code>email@example.com</code> inside the container <code>-e \"PGADMIN_DEFAULT_PASSWORD=password\"</code> Sets an environment variable <code>PGADMIN_DEFAULT_PASSWORD</code> with the value <code>password</code> inside the container <code>-d</code> Runs the container in the background (detached mode) <code>dpage/pgadmin4:latest</code> Specifies the name and tag of the pgAdmin4 Docker image to use for the new container <p>Info</p> <p>You'll notice that this time, the version is set using :latest, this is a tag that refers to the most recent version of a Docker image. It's better to specify a version like dpage/pgadmin4:4.32 this would ensure that you always use the same version of the image, regardless of whether a new \"latest\" version is released.</p> <p> Now, if you go to localhost:5050 you'll be able to access the pg admin interface. </p> <p></p> <p>If you perform a <code>docker ps</code> you'll see two containers, one for postgres and one for pgadmin</p> <pre><code>CONTAINER ID   IMAGE               COMMAND                  CREATED         STATUS         PORTS                                       NAMES\n3ca9e24601b5   postgres:15.2      \"docker-entrypoint.s\u2026\"   5 minutes ago   Up 5 minutes   0.0.0.0:5432-&gt;5432/tcp, :::5432-&gt;5432/tcp   postgres\na08f7a258dbf   dpage/pgadmin4:4.23 \"/entrypoint.sh\"         5 minutes ago   Up 5 minutes   0.0.0.0:5050-&gt;80/tcp, :::5050-&gt;80/tcp       pgadmin\n</code></pre> <p></p>"},{"location":"practice/docker/#deploy-a-bi-platform-metabase","title":"Deploy a BI platform, metabase","text":"<p>There are many BI platforms, however only few are open-source. In addition, metabase can be deployed in seconds...  with docker.  <pre><code>docker run -d -p 3000:3000 --name metabase metabase/metabase\n</code></pre>  If you perform a docker ps you'll see 3 containers :  <pre><code>CONTAINER ID   IMAGE               COMMAND                  CREATED         STATUS         PORTS                                       NAMES\n3ca9e24601b5   postgres:15.2      \"docker-entrypoint.s\u2026\"   5 minutes ago   Up 5 minutes   0.0.0.0:5432-&gt;5432/tcp, :::5432-&gt;5432/tcp   postgres\na08f7a258dbf   dpage/pgadmin4:4.23 \"/entrypoint.sh\"         5 minutes ago   Up 5 minutes   0.0.0.0:5050-&gt;80/tcp, :::5050-&gt;80/tcp       pgadmin\ncc8a7f165c43   metabase/metabase:0.45 \"/app/run_metabase.s\u2026\"  5 seconds ago    Up 4 seconds    0.0.0.0:3000-&gt;3000/tcp                     metabase\n</code></pre></p> <p> Metabase is now accessible on localhost:3000. You'll have to wait a bit of time to be able to access it and have  a page like this:  </p> <p></p> <p></p>"},{"location":"practice/docker/#all-together-docker-compose","title":"All together, docker-compose","text":"<p>We've seen how to run single container application, using separated commands, but this is not very convenient... As previously mentioned, there is a way to manage multi-container applications : <code>docker-compose</code></p> <p>Info</p> <p>You will not have to learn commands for both <code>docker</code> and <code>docker-compose</code>, basically you'll find the  same commands. Ex : <code>docker ps</code> become <code>docker-compose ps</code></p> <p>In order to use docker-compose, you need to write a <code>docker-compose.yml</code> file that describe each container you want to  deploy.</p> <p>Here is an example of a file describing our 3 previously deployed application : </p> <p></p> <pre><code>version: \"3\"\nservices:\npostgres:\nimage: postgres:14.2-alpine\nrestart: always\nenvironment:\nPOSTGRES_PASSWORD: postgres\nPOSTGRES_USER: postgres\nPOSTGRES_HOST: \"0.0.0.0\"\nvolumes:\n- postgres:/var/lib/postgresql/data\npgadmin:\nimage: dpage/pgadmin4:4.23\nenvironment:\nPGADMIN_DEFAULT_EMAIL: admin@pgadmin.com\nPGADMIN_DEFAULT_PASSWORD: password\nPGADMIN_LISTEN_PORT: 80\nports:\n- 8080:80\nvolumes:\n- pgadmin:/var/lib/pgadmin\ndepends_on:\n- postgres\nmetabase:\nimage: metabase/metabase\nports:\n- \"3000:3000\"\nrestart: always\nvolumes:\npostgres:\npgadmin:\n</code></pre> <p>Inside the folder where you <code>docker-compose.yml</code> file is located, simply run <code>docker-compose up -d</code>.</p> <p>Info</p> <ul> <li>The version \"3\" at the beginning of the file indicates that this is a Docker Compose file written in version 3 syntax.</li> <li>The file defines three services: postgres, pgadmin, and metabase.</li> <li>The postgres service runs the official PostgreSQL 14.2-alpine image, which is a lightweight version of the PostgreSQL database running on the Alpine Linux distribution. It sets environment variables for the database user and password, and also specifies a volume to store the database data.</li> <li>The pgadmin service runs the dpage/pgadmin4:4.23 image, which is a web-based administration tool for PostgreSQL. It sets environment variables for the default email and password, and exposes port 8080 on the host machine to port 80 on the container. It also specifies a volume to store pgAdmin data, and depends on the postgres service to be running.</li> <li>The metabase service runs the metabase/metabase image, which is a business intelligence and analytics tool. It exposes port 3000 on the host machine to port 3000 on the container, and sets the restart policy to always.</li> <li>The volumes section defines two named volumes, postgres and pgadmin, which are used by the postgres and pgadmin services respectively to store data.</li> </ul>"}]}