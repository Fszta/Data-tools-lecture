{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#some-general-reading","title":"Some general reading ...","text":"<p>Question</p> <p>What is data science?</p> <p>Data science is an interdisciplinary field that involves the extraction of insights from data using statistical, computational, and machine learning techniques. Data science involves the entire process of collecting, processing, analyzing, and interpreting data to solve complex problems and make data-driven decisions.</p> <p>Question</p> <p>Why is data science important?</p> <p>Data science is important because it allows us to make sense of vast amounts of data that are generated every day. By extracting insights from data, we can identify patterns, trends, and relationships that can inform business decisions, scientific research, and public policy. Data science also plays a critical role in developing and improving machine learning algorithms that power many modern technologies.</p> <p>Question</p> <p>What are some common applications of data science?</p> <p>Data science is used in a wide range of industries and fields, including business, healthcare, finance, marketing, and scientific research. Some common applications of data science include fraud detection, customer segmentation, predicting disease outbreaks, personalized recommendations, and image and speech recognition.</p> <p>Question</p> <p>What are data science tools?</p> <p>Data science tools are software applications, libraries, and frameworks that are used to facilitate the various tasks involved in data science, such as data manipulation, data analysis, machine learning, and data visualization.</p>"},{"location":"#session-1","title":"Session 1","text":"<pre><code>    %%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'showBranches': true, 'showCommitLabel':true,'mainBranchName': 'session1'}} }%%\n\n    gitGraph\n       checkout session1\n       commit id: \"Git &amp; Version control\"\n       commit id: \"Python package management\"\n       branch foo\n       checkout foo\n       commit id: \"Anaconda + Conda\"\n       commit id: \"pip\"\n       commit id: \"poetry\"\n       checkout session1\n       merge foo\n       commit id: \"Notebooks\"\n</code></pre>"},{"location":"#session-2","title":"Session 2","text":"<pre><code>    %%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'showBranches': true, 'showCommitLabel':true,'mainBranchName': 'session2'}} }%%\n\n    gitGraph\n       commit id: \"1\"\n       branch docker\n       checkout docker\n       commit id: \"postgres\"\n       commit id: \"pgadmin\"\n       commit id: \"metabase\"\n       checkout session2\n       merge docker\n       commit id: \"2\"\n       branch docker-compose\n       checkout docker-compose\n       commit id: \"all at once\"\n       checkout session2\n       merge docker-compose\n\n\n</code></pre>"},{"location":"#session-3","title":"Session 3","text":"<p>This session will be oriented around airflow.</p>"},{"location":"#session-4","title":"Session 4","text":"<p>The last session will be focus on data visualization with <code>plotly dash</code>. We will also apply all the notion related to git &amp; docker we've previously seen</p>"},{"location":"airflow/","title":"Airflow","text":"<p>Airflow is an open-source platform initially develop by Airbnb to schedule, and orchestrate  of complex data workflows. It is primarily used in the field of data engineering and data processing to manage the  movement and transformation of data between systems and processes.</p> <p>At its core, Airflow provides a workflow automation framework that allows users to define, execute,  and monitor data pipelines as directed acyclic graphs (DAGs).</p> <p>Info</p> <p>You can also use some alternative tool, even if airflow is used in many companies, some of them chose to use  similar tool : </p> <ul> <li>Prefect</li> <li>Luigi</li> <li>Dagster</li> <li>...</li> </ul>"},{"location":"airflow/#whats-a-dag","title":"What's a dag ?","text":"<p>DAGs are graphs that represent the dependencies and  relationships between tasks, where tasks are units of work that perform specific actions on data,  such as data extraction, transformation, or loading. DAGs in Airflow are defined using Python.</p> <p></p>"},{"location":"airflow/#why-do-i-need-such-a-tool","title":"Why do I need such a tool ?","text":"<p>In the data-world, and especially when you're building an analytics stack you'll perform three main types of operations * Extract data from sources : it can be API, Databases, files, using scraping technics etc... * Transform the data : you'll probably clean it, perform some aggregation, consolidate it with other sources * Load the data : you'll put the data in an appropriate system depending on your needs, for example in a warehouse   and plug a bi tool on top of it. You will also use it to expose the data to other systems / applications (ML etc...)</p> <p>To orchestrate all those operations and implement the logic between them, you'll need some tool : airflow is one of them.</p> <p>You will meet two main architectural pattern : ETL &amp; ELT. Even if they look very similar, they are not and they have their own pros and cons</p>"},{"location":"airflow/#recap-data-integration-pattern","title":"Recap : Data integration pattern","text":""},{"location":"airflow/#etl","title":"ETL","text":"<p>ETL is the more \"traditional\" approach, where you prepare your data before loading it to your target system. It means that the workflow is performed by the transformation engine.</p> <p></p>"},{"location":"airflow/#elt","title":"ELT","text":"<p>ELT is the more modern approach, the main point here is that you load the data first, and process the data after. In this case, most the computation is done on the target system directly, which act as the transformation  engine. This is a common approach used when your core component is a warehouse, like snowflake of bigquery, designed to handle intensive computation</p> <p></p>"},{"location":"airflow/#how-does-it-work","title":"How does it work ?!","text":"<p>Tip</p> <p>It's not mandatory to be able to explain how airflow works to use it, however it's a good thing to have a good  understanding of the tool you're using...</p> <p>As a user, you will interact with 3 components : </p> <ul> <li>You will configure airflow : writing the <code>airflow.cfg</code> file</li> <li>You will use the UI :  for managing and monitoring workflows. It allows you to interact with Airflow,  such as viewing DAGs, triggering DAG runs, monitoring task statuses, and managing connections, variables etc...  </li> <li>You will write python DAGs</li> </ul> <p>But there are not the only required components... Here is a brief explanation of the role of each component :</p> <ul> <li> <p>Worker: A worker is a component in the Airflow architecture that is responsible for executing tasks.  When a task is triggered for execution, it is assigned to a worker, which runs the task and reports the result back  to the scheduler. Workers can be horizontally scaled to handle a large number of tasks concurrently,  making them suitable for parallel processing of tasks.</p> </li> <li> <p>Scheduler The scheduler is the core component of Airflow that manages the scheduling and execution of tasks.  It determines when tasks should be executed based on their dependencies, triggers, and time-based schedules defined in  the DAGs (directed acyclic graphs). The scheduler communicates with the metadata database to keep track of task status,  schedule changes, and other metadata. It also communicates with workers to assign tasks for execution and monitor their  progress.</p> </li> </ul> <p></p> <p>Metadata Database: Airflow uses a SQL database to store metadata about the data pipelines being run. In the diagram above, this is represented as Postgres which is extremely popular with Airflow.</p> <p>Extract from airflow documentation</p>"},{"location":"airflow/#how-to-deploy-airflow","title":"How to deploy airflow ?","text":"<p>There are several way to deploy airflow, the choice will highly depends on your requirements !</p> <p>Warning</p> <p>Be careful, some deployments are not suitable for production</p>"},{"location":"airflow/#deploy-airflow-locally","title":"Deploy airflow locally","text":"<p>Here the link to the airflow documentation explaining how to easily deploy it on your local machine.</p> <p>Info</p> <p>You will probably use it for development !</p>"},{"location":"airflow/#using-a-cloud-managed-service","title":"Using a cloud managed service","text":"<p>Depending on the provider you're working on, you'll use different service with their specificities. One of the biggest difference will be the way of providing your dag to the service.</p>"},{"location":"airflow/#on-aws","title":"On AWS","text":"<p>Aws has developed a service called MWAA built on top of airflow, it allows you to not taking care of the management  of the underlying infrastructure... which can be great when you don't have the resources / knowledges !</p> <p>The only major difference for a developer is the way of providing your dags, with MWAA you'll need to push them to S3 </p> <p></p> <p>Extract from aws documentation</p>"},{"location":"airflow/#on-gcp","title":"On GCP","text":"<p>Google has developed Cloud composer, also built on top of airflow.</p>"},{"location":"airflow/#deploy-airflow-on-kubernetes-on-premise-or-in-a-cloud-environment","title":"Deploy airflow on kubernetes (on-premise or in a cloud environment)","text":"<p>Info</p> <p>You can manage a kubernetes cluster ... or use the managed service of your cloud provider.</p> <p>Airflow can be deployed on Kubernetes, a popular container orchestration platform, which provides advanced capabilities for managing containerized applications at scale. Deploying Airflow on Kubernetes allows for dynamic scaling,  rolling updates, and self-healing of Airflow components, making it suitable for large-scale and production-grade  workflows.</p>"},{"location":"airflow/#how-to-create-a-dag","title":"How to create a dag","text":"<p>As previously mentioned, a dag can be written in python... and it's simple !</p>"},{"location":"airflow/#airflow-operators","title":"Airflow operators","text":"<p>We've seen that a dag is an association of tasks, with dependencies, but what is a task ? A task can be the execution of a python function, some shell commands, an SQL request etc...</p> <p>The great thing here is that airflow come with some pre-built <code>Operators</code> that you can use in your tasks.</p> <ul> <li><code>BashOperator</code> - executes a bash command</li> <li><code>PythonOperator</code> - calls a Python function</li> <li><code>PostgresOperator</code> - To execute an SQL request on a postgres database</li> <li><code>MySqlOperator</code> - To execute an SQL request on a mysql database</li> <li>... A lot of operator exists, and allows you to interact with external system easily</li> </ul> <p></p>"},{"location":"airflow/#example-a-first-minimal-dag","title":"Example - A first minimal dag","text":"<pre><code>from datetime import datetime\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\ndef print_hello_world():\nreturn 'Hello world !'\nwith DAG('hello_world',\ndescription='Hello world dag',\nschedule_interval='0 1 * * *',\nstart_date=datetime(2023, 4, 10)\n) as dag:\nprint_hello_operator = PythonOperator(task_id='hello_task', python_callable=print_hello_world, dag=dag)\nprint_hello_operator\n</code></pre> <p>Info</p> <p>We can now visualize our dag on the airflow UI. By default, any dag is not activated, we need to switch the left  toggle to activate it, and allow the scheduler to trigger it.</p> <p></p> <p>We've defined, a dag containing a single task, it's not really useful in the real world. Let's create an other examples with multiple tasks.</p>"},{"location":"airflow/#example","title":"Example","text":"<p>Assuming we want to create a daily job that move some files from one source directory to a target directory : </p> <ul> <li>Let's define a function that list the files in the source</li> <li>Implement another function that task as parameter the files from task1 and move the files from the source to the destination</li> </ul> <pre><code>import os\nimport shutil\nfrom datetime import datetime\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\ndef get_files_in_directory(directory_path: str) -&gt; list[str]:\nreturn os.listdir(directory_path)\ndef move_files(files_name: list[str], source_directory: str, target_directory: str) -&gt; None:\nfor file_name in files_name:\n# Construct source and target file paths\nsource_file = os.path.join(source_directory, file_name)\ntarget_file = os.path.join(target_directory, file_name)\n# Move the file from source to target\nshutil.move(source_file, target_file)\nwith DAG('move_file_dag', schedule_interval='0 1 * * *', start_date=datetime(2023, 4, 13)) as dag:\nget_files_task = PythonOperator(\ntask_id='get_files_task',\nop_args=[\"./sources\"],\npython_callable=get_files_in_directory,\ndag=dag\n)\nmove_files_task = PythonOperator(\ntask_id='move_files_task',\npython_callable=move_files,\nop_args=[get_files_task.output, \"./sources\", \"./destination\"],  # Pass output of get_files_task as argument\ndag=dag\n)\nget_files_task &gt;&gt; move_files_task\n</code></pre> <p>Info</p> <p>As you can see, to define the relationship between the two task, we use <code>&gt;&gt;</code> operator, you can also wrap multiple tasks between square brackets to be dependent on the same upstream task. For example :  <code>first_task &gt;&gt; second_task &gt;&gt; [third_task, fourth_task]</code></p>"},{"location":"airflow/#successful-dag-execution","title":"Successful dag execution","text":"<p>In case of success, you should have all your tasks in green :</p> <p></p>"},{"location":"airflow/#failing-dag-execution","title":"Failing dag execution","text":"<p>If the first task <code>get_files_task</code> fails, the second task <code>move_files_task</code> will not be executed (with the default config)  Here a visual example of this case : </p> <p></p>"},{"location":"airflow/#some-interesting-features","title":"Some interesting features","text":"<p>We've seen some very basic features of airflow, it has a lot of other features, that can help you to build some more complex pipeline.</p>"},{"location":"airflow/#callback","title":"Callback","text":"<p>A <code>Callback</code> is a function that is only invoked when the task state changes due to execution by a worker. For example, when a task fails. </p> <p>Tip</p> <p>One good example of callback use is to send some alert when a task fail, it's very common to trigger an alert sent by slack or by email to inform that a task has fail.</p> <p>There are fours type of callback : </p> Name Description on_success_callback Invoked when the task succeeds on_failure_callback Invoked when the task fails sla_miss_callback Invoked when a task misses its defined SLA on_retry_callback Invoked when the task is up for retry <p>From airflow documentation</p>"},{"location":"airflow/#connections","title":"Connections","text":"<p>Connections are used for storing credentials that are used to connect to external systems such as databases.</p> <p>A Connection is essentially set of parameters - such as username, password and hostname -  along with the type of system that it connects to, and a unique name, called the conn_id.</p> <p>They can be managed via the UI or via the CLI.</p> <p>Via the UI, you'll need to go to <code>Admin&gt;&gt;Connections</code> and will have the following page:</p> <p></p> <p>You'll need to install some additional dependencies to use some pre-built connections. For postgresql, you need to  install the package : <code>pip install apache-airflow-providers-postgres</code> </p> <p>Warning</p> <p>After installing a dependency, you'll need to restart your airflow instance</p>"},{"location":"airflow/#hooks","title":"Hooks","text":"<p>A hook is an interface that allow you to easily communicate with an external system (database for example), they use the connections to connect to those system. As they encapsulate the details of connecting to and interacting  with external systems, it helps you to perform some operations with a minimal amount of code.</p> <p>Tip</p> <p>If you need to interact with a Postgres database, have a look to the <code>PostgresHook</code></p>"},{"location":"airflow/#cron-format-reminder","title":"Cron format reminder","text":"<p>If you have any doubt on your schedule format, check it on cron-guru</p> <p>Cron format</p> <pre><code>* * * * *\n- - - - -\n| | | | |\n| | | | ----- Day of week (0 - 7) (Sunday is both 0 and 7)\n| | | ------- Month (1 - 12)\n| | --------- Day of month (1 - 31)\n| ----------- Hour (0 - 23)\n------------- Minute (0 - 59)\n</code></pre>"},{"location":"airflow/#example_1","title":"Example","text":"<ul> <li>To run a command every day at 3:30 AM: <code>30 3 * * *</code></li> <li>To run a command every week on Sunday at 8:00 PM: <code>0 20 * * 0</code></li> <li>To run a command every month on the 15th at 1:45 PM: <code>45 13 15 * *</code></li> </ul>"},{"location":"data-visualization/","title":"Data visualization","text":""},{"location":"data-visualization/#introduction","title":"Introduction","text":"<p>There are many ways to provide some data visualization : </p> <ul> <li>Using some libraries like matplotlib, seaborn or plotly</li> <li>Using a BI platform (Tableau, Metabase, PowerBI) ...</li> <li>Developing it from scratch </li> </ul> <p>Info</p> <p>This is only a subset of examples.</p> <p>Each solution comes with some pros &amp; cons and the choice will highly depends on your requirements. For example, matplotlib and seaborn may not be the right choice if you want to expose some interactive dashboard that  can be rendered in a web-browser and be easily shared. Developing it from scratch can be too much time-consuming if you don't have advanced skills in web development (which is probably the case if you're a data guy).</p> <p>One good solution for someone coming from the data field is to use a framework like <code>Plotly Dash</code>. It allows you to create dynamic and interactive web-based application using python code. Also, it  allows you to manipulate the data using <code>pandas</code> which make it a good choice for people from the data field.</p>"},{"location":"data-visualization/#plotly-dash","title":"Plotly Dash","text":"<p>Plotly Dash is described by its developers as <code>The original low-code framework for rapidly building data apps  in Python.</code></p> <p>The main advantage of this framework is that you don't have to write extensive html, css or javascript. In addition, it follows a reactive programming paradigm, which basically mean that changes in the user interface trigger changes in the data, and changes in the data trigger changes in the UI.</p> <p>Here is an example of what you can build with plotly dash </p>"},{"location":"data-visualization/#installation","title":"Installation","text":"<p>It is very straightforward to install ! Simply run <code>pip install dash</code> and you're done.</p>"},{"location":"data-visualization/#how-to-use-it","title":"How to use it ?","text":"<p>Dash come with a set of components that are ready to use. </p> <ul> <li>html components : Instead of writing HTML or using an HTML templating engine, you compose your layout using Python with the  Dash HTML Components module <code>dash.html</code>.</li> <li>core components. The Dash Core Components module <code>dash.dcc</code> gives you access to many interactive components,  including graphs, dropdowns, checklists, and sliders.</li> </ul>"},{"location":"data-visualization/#hello-world","title":"Hello world","text":"<pre><code>from dash import Dash, html\napp = Dash(__name__)\napp.layout = html.Div(children=[\nhtml.H1(\"My first dash application\")\n])\nif __name__ == '__main__':\napp.run_server(debug=True)\n</code></pre> <p>What really happens is that this code is converted to the following <code>html</code>:</p> <pre><code>&lt;div&gt;\n&lt;h1&gt;My first dash application&lt;/h1&gt;\n&lt;/div&gt;\n</code></pre> <p>After running the code, you can access your dashboard at http://127.0.0.1:8050/</p> <p>Info</p> <p>You define your application <code>app = Dash(__name__)</code> and set its layout as being an html <code>div</code>. To add some components to your dashboard, you just need to add them as element of the parents.children list.</p>"},{"location":"data-visualization/#example-with-two-html-components","title":"Example with two html components","text":"<p>Let's add an H2 title, we define it as a children of the parent div. <pre><code>from dash import Dash, html\napp = Dash(__name__)\napp.layout = html.Div(children=[\nhtml.H1(\"My first dash application\"),\nhtml.H2(\"It's a title2\")\n])\nif __name__ == '__main__':\napp.run_server(debug=True)\n</code></pre></p> <p>You can find all the available HTML components on the following page</p>"},{"location":"data-visualization/#adding-some-custom-style","title":"Adding some custom style","text":"<p>You can add some custom css style to any components simply by passing to the component, a variable which contains a  dictionary containing the style properties you want to apply.  If you want to change the color of the titles :</p> <pre><code>from dash import Dash, html\napp = Dash(__name__)\napp.layout = html.Div(children=[\nhtml.H1(\"My first dash application\", style={'color': '#ADD8E6'}),\nhtml.H2(\"It's a title2\", style={'color': '#00008B'})\n])\nif __name__ == '__main__':\napp.run_server(debug=True)\n</code></pre> <p>You can also use classic css file, you only need to put it in an <code>assets</code> directory at the same level as your  application file</p> <pre><code>- app.py\n- assets/\n    |-- your_style1.css\n    |-- your_style_2.css\n</code></pre> <p>Instead of passing the <code>style</code> variable in the previous example, we can add the following file :</p> <pre><code>h1 {\ncolor: #ADD8E6\n}\nh2 {\ncolor: #00008B\n}\n</code></pre>"},{"location":"data-visualization/#work-with-some-real-data-graphs","title":"Work with some real data &amp; graphs","text":"<p>Tip</p> <p>As previously mentioned, plotly allow you to pass it some data from a pandas dataframe, which makes really easy to work with csv data.</p> <p>Assuming we have the following input data (we will create it in the code) : </p> Category Value A 10 B 20 C 15 D 5 <p>We want to create a barchat which represents the value foreach category. It can be done by adding a new children <code>dcc.Graph()</code> in the parent component. </p> <p>There are multiple way to create the figure of the graph : </p> <ul> <li>using plotly express api (the most straightforward)</li> <li>using list and dict</li> <li>using <code>go.Figure</code></li> </ul> <p>Let's do it with plotly express : </p> <p>You'll be able to create a barchat, with 2 line of code :  <pre><code>dcc.Graph(\nid='bar-chart',\nfigure=px.bar(df, x='Category', y='Value')\n)\n</code></pre></p> <p>The full example : </p> <pre><code>import pandas as pd\nimport plotly.express as px\nfrom dash import Dash, html, dcc\napp = Dash(__name__)\n# Create a dataframe with fake data (you can use data from a csv or any source instead...)\ndf = pd.DataFrame({'Category': ['A', 'B', 'C', 'D'],\n'Value': [10, 20, 15, 5]})\napp.layout = (html.Div(children=[\nhtml.H1(\"My first graph\"),\ndcc.Graph(\nid='bar-chart',\nfigure=px.bar(df, x='Category', y='Value')\n)\n]))\nif __name__ == '__main__':\napp.run_server(debug=True)\n</code></pre> <p> If you want to do it with dict and list only :</p> <pre><code>import pandas as pd\nfrom dash import Dash, html, dcc\napp = Dash(__name__)\n# Create a dataframe with fake data (you can use data from a csv or any source instead...)\ndf = pd.DataFrame({'Category': ['A', 'B', 'C', 'D'],\n'Value': [10, 20, 15, 5]})\napp.layout = (html.Div(children=[\nhtml.H1(\"My first graph\"),\ndcc.Graph(\nid='my_first_barchart',\nfigure={\n'data': [{'x': df['Category'], 'y': df['Value'], 'type': 'bar'}],\n'layout': {'title': 'My first barchart!'}\n}\n)\n]))\nif __name__ == '__main__':\napp.run_server(debug=True)\n</code></pre>"},{"location":"data-visualization/#basic-callbacks","title":"Basic callbacks","text":"<p>What's a callback ?</p> <p>callbacks are functions that are automatically called by Dash whenever an input component's property changes,   in order to update some property in another component (the output).</p> <p>To achieve filtering (with a dropdown, an input value or whatever), you'll need to define callback to update the graph. Let's use the same input data (category / value df). We want to be able to filter for specific category,  to do so, we will add a dropdown filter :</p> <pre><code> dcc.Dropdown(\nid='category-filter',\noptions=[{'label': category, 'value': category} for category in df['Category']],\nvalue=None,\nplaceholder='Select a category'\n)\n</code></pre> <p>Now, we want to update our barchart when a category is selected in the dropdown -&gt; Let's implement a callback</p> <p>Info</p> <p>What we do here is that we define a fonction that return a figure (our barchart) based on the filtered dataframe. The specificity is the <code>annotator</code> used to specify which chart are affected (using their id) and what's the input (the value of the category-filter)</p> <pre><code>@app.callback(\ndependencies.Output('bar-chart', 'figure'),\ndependencies.Input('category-filter', 'value')\n)\ndef update_bar_chart(category):\nif category is None:\n# Keep all categories if no value has been selected\nfiltered_df = df\nelse:\n# Filter the df based on selection\nfiltered_df = df[df['Category'] == category]\nreturn px.bar(filtered_df, x='Category', y='Value')\n</code></pre> <p>Tip</p> <p>You can use the same callback to update multiple graphs ! In such a case, you function must return as many figure as the number of graphs you want to update</p>"},{"location":"data-visualization/#here-is-the-full-example","title":"Here is the full example","text":"<pre><code>import pandas as pd\nimport plotly.express as px\nfrom dash import Dash, html, dcc, dependencies\ndf = pd.DataFrame({'Category': ['A', 'B', 'C', 'D'],\n'Value': [10, 20, 15, 5]})\napp = Dash(__name__)\napp.layout = html.Div(\nchildren=[\nhtml.H1('My First Dash App'),\ndcc.Dropdown(\nid='category-filter',\noptions=[{'label': category, 'value': category} for category in df['Category']],\nvalue=None,\nplaceholder='Select a category'\n),\ndcc.Graph(\nid='bar-chart',\nfigure=px.bar(df, x='Category', y='Value')\n)\n]\n)\n# Define callback for updating the bar chart based on the category filter\n@app.callback(\ndependencies.Output('bar-chart', 'figure'),\ndependencies.Input('category-filter', 'value')\n)\ndef update_bar_chart(category):\nif category is None:\n# Keep all categories if no value has been selected\nfiltered_df = df\nelse:\n# Filter the df based on selection\nfiltered_df = df[df['Category'] == category]\nreturn px.bar(filtered_df, x='Category', y='Value')\nif __name__ == '__main__':\napp.run_server(debug=True)\n</code></pre>"},{"location":"data-visualization/#advanced-visualization-maps","title":"Advanced visualization - Maps","text":"<p>There are a lot of visualization in plotly dash, let's see an example on how can we draw a map based on some coordinates ! </p> <p>You can do it by using <code>px.scatter_mapbox()</code> to generate a map figure, you need to specify <code>lat</code> &amp; <code>lon</code> column name  and your dataframe. </p> <p>Tip</p> <p>You can apply many different customization, such has changing the style of the map, setting the rendered values when hover over a data point etc... Also you can add some filters as we've seen previously, it also works with maps.</p> <p>lat &amp; lng need to be valid numerical values</p> <p><pre><code>import plotly.express as px\nimport pandas as pd\nfrom dash import Dash, html, dcc\napp = Dash(__name__)\ndf = pd.DataFrame({'City': ['Paris', 'New York', 'Los Angeles', 'Tokyo'],\n'Lat': [48.8566, 40.7128, 34.0522, 35.6895],\n'Lon': [2.3522, -74.0060, -118.2437, 139.6917],\n'Value': [10, 20, 15, 5]})\napp.layout = html.Div(children=[\nhtml.H1(\"My first map\", style={'color': '#ADD8E6'}),\ndcc.Graph(id=\"map-graph\", figure=px.scatter_mapbox(\ndf, \nlat='Lat',\nlon='Lon',\nhover_name='City',\nzoom=1\n).update_layout(mapbox_style='open-street-map'))\n])\nif __name__ == '__main__':\napp.run_server(debug=True)\n</code></pre> This will render :</p> <p></p>"},{"location":"docker/","title":"Docker","text":""},{"location":"docker/#introduction","title":"Introduction","text":"<p>Docker is a software platform that allows developers to package, distribute, and run applications in a containerized  environment. Containers are lightweight, portable, and self-contained environments that can run almost anywhere,  from a developer's laptop to a cloud-based infrastructure.</p>"},{"location":"docker/#what-is-containerization","title":"What is containerization ?","text":""},{"location":"docker/#key-concepts-components","title":"Key concepts &amp; components","text":"<p>Tip</p> <p>The best way to be familiar with docker is to practice, also they provide a great documentation</p>"},{"location":"docker/#1-dockerfile","title":"1. Dockerfile","text":"<p>A Dockerfile is a text file that contains instructions on how to build a Docker image. The Docker image is a binary file  that contains everything needed to run an application, including the code, libraries, and dependencies. </p>"},{"location":"docker/#2-docker-image","title":"2. Docker image","text":"<p>A Docker image is a snapshot of a container, which includes the application code, libraries, and dependencies.  Images can be built from a Dockerfile or pulled from a public or private Docker registry.</p>"},{"location":"docker/#3-docker-registry","title":"3. Docker registry","text":"<p>A Docker registry is a repository for storing and sharing Docker images. Docker Hub is the most popular public Docker  registry, but you can also use private registries for your organization's images.</p>"},{"location":"docker/#4-docker-container","title":"4. Docker container","text":"<p>A Docker container is a lightweight, standalone executable package that includes everything needed to run an  application, including the application code, libraries, and dependencies. Containers are isolated from the host  system and from other containers, making them a secure way to run applications.</p>"},{"location":"docker/#5-docker-compose","title":"5. Docker Compose","text":"<p>Docker Compose is a tool for defining and running multi-container Docker applications. With Docker Compose, you can define the services that make up your application, their configuration, and the network they should use to communicate with each other.</p>"},{"location":"docker/#6-docker-swarm","title":"6. Docker Swarm","text":"<p>Docker Swarm is a native clustering and orchestration tool for Docker. With Swarm, you can create and manage a cluster  of Docker nodes, and deploy and manage Docker services across the cluster.</p> <p>Info</p> <p>It's not very common to use it in a production environment, the standard is to use kubernetes</p>"},{"location":"docker/#how-to-build-a-docker-image-for-my-application","title":"How to build a docker image for my application ?","text":"<p>Tip</p> <p>The convention is to provide the configuration in a <code>Dockerfile</code> file, however you can give choose another name  instead of the default one.     </p> <p>To distribute your application with docker, you basically need to provide the following information and assets:</p> <ul> <li>On which environment my image must be based on ? It can be python, ubuntu. This is done using the <code>FROM</code> keyword</li> <li>What will be the working directory ? This is done using <code>WORKDIR</code> keyword</li> <li>Provide the source files (for example your python code) of the application and any needed file. This is done using  <code>COPY</code> keyword. It works as follows : <code>COPY &lt;src&gt; &lt;dest&gt;</code> </li> <li>Provide the command that must be run during the build process (for example installing python dependencies).  This is done using <code>RUN</code> keyword</li> <li>Specify the port that your container will listen on at runtime. For example, if you deploy an API listening on port <code>8080</code>, you need to expose this port to make it accessible from outside the container</li> <li>Finally, you need to specify the command to run when a container is started. This is done using <code>CMD</code> keyword and  has the following syntax : <code>CMD [\"executable\",\"param1\",\"param2\"]</code> </li> </ul> <p>There are many other keywords that can be used, for example <code>ENV</code> to set environment variables or <code>ARG</code> to handle input argument, you can find the list in docker documentation</p>"},{"location":"docker/#example-docker-image-for-a-flask-application","title":"Example - Docker image for a flask application","text":"<p>Assuming you have the following hello-world flask api, in a file <code>app.py</code> : </p> <pre><code>from flask import Flask\napp = Flask(__name__)\n@app.route('/')\ndef hello():\nreturn 'Hello, World!'\nif __name__ == '__main__':\napp.run(host='0.0.0.0', port=5000)\n</code></pre> <p>Your <code>requirements.txt</code> contains :  <pre><code>Flask==2.2.3\n</code></pre></p> <p>Here the corresponding <code>Dockerfile</code> : </p> <p><pre><code># Use the official Python base image\nFROM python:3.9\n# Set the working directory of the container\nWORKDIR /app\n# Copy the requirements file containing your dependencies list into the container\nCOPY requirements.txt .\n\n# Install necessary dependencies\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy the code of the app (there is only one file), can copy all current dir using `.`\nCOPY app.py .\n\n# Expose port 5000, the default port used by flask\nEXPOSE 5000\n# Start the Flask app\nCMD [\"python\", \"app.py\"]\n</code></pre> Now, you can build the docker image (the following command will work if you are in the directory containing your  Dockerfile) : <code>docker build -t flask-app-example .</code></p> <p>You should have an output like :</p> <pre><code>Sending build context to Docker daemon  123.45kB\nStep 1/5 : FROM python:3.9\n3.9: Pulling from library/python\n...\nStatus: Downloaded newer image for python:3.9\n ---&gt; 1234567890ab\nStep 2/5 : WORKDIR /app\n ---&gt; Running in 9876543210ba\nRemoving intermediate container 9876543210ba\n ---&gt; fedcba987654\nStep 3/5 : COPY requirements.txt .\n ---&gt; 0123456789cd\nStep 4/5 : RUN pip install --no-cache-dir -r requirements.txt\n...\nSuccessfully installed Flask-1.1.2 ...\nStep 5/5 : COPY app.py .\n ---&gt; 9876543210dc\nRemoving intermediate container 9876543210dc\n ---&gt; fedcba987654\nSuccessfully built fedcba987654\nSuccessfully tagged flask-app-example:latest\n</code></pre> <p>You can list the docker images on your machine with <code>docker image ls</code>. You must be able to see the previously created image :  <pre><code>REPOSITORY              TAG       IMAGE ID       CREATED       SIZE\nflask-app-example       latest    fedcba987654   2 hours ago   123MB\n</code></pre></p> <p>Info</p> <p>As we've not set any tag, the default <code>latest</code> will be used, it indicates that this is the latest version.</p>"},{"location":"docker/#run-your-application","title":"Run your application !","text":"<p>You can now run your application using <code>docker run -p 5000:5000 -d flask-app-example</code></p>"},{"location":"docker/#conclusion","title":"Conclusion","text":"<p>If you want to run your application in any cloud environment, you need to push the docker image into a docker registry. Each cloud provider has its own container registry service. You can do it manually from your local  machine using the cli of your cloud provider, however it's important to keep in mind that you will not do it manually in a real project. The process of build and distribution of the docker image will be done in your <code>CI/CD</code> pipeline.</p>"},{"location":"docker/#commands-recap","title":"Commands recap","text":"<p>Tip</p> <p>Once again, you don't need to remind all the commands, just practice and go through the documentation ! </p> Command Description docker run Run a container docker ps List running containers docker images List available images docker build Build an image from a Dockerfile docker push Push an image to a remote registry docker pull Pull an image from a remote registry docker stop Stop a running container docker rm Remove a container docker rmi Remove an image docker exec Execute a command in a running container docker logs View the logs of a container <p>Warning</p> <p>In this page, we only cover the basics of docker, the best way to be more confortable with docker is to practice  through your student projects !</p>"},{"location":"git/","title":"Git","text":""},{"location":"git/#what-we-have-seen-last-week","title":"What we have seen last week","text":"<ul> <li>git concept &amp; basic commands</li> <li>branches</li> <li>pull request &amp; review process</li> <li>simple feature branch workflow</li> <li>ci/cd introduction</li> <li>zoom on github</li> </ul>"},{"location":"git/#commands-recap","title":"Commands recap","text":"<p>Tip</p> <p>You don't need to remind all the commands, just practice and go through the documentation or man page !</p> Command Description <code>git init</code> Initialize a git repository <code>git add &lt;file_name&gt;</code> Add a file to staging area <code>git commit -m \"an explicit message\"</code> Commit changes with an explicit message <code>git push origin &lt;branch_name&gt;</code> Push changes to a remote branch <code>git branch &lt;branch_name&gt;</code> Create a new branch <code>git checkout &lt;branch_name&gt;</code> Switch to a branch <code>git checkout -b &lt;branch_name&gt;</code> Create and switch to a new branch"},{"location":"git/#branch-coding-workflow","title":"Branch &amp; coding workflow","text":"<p>Danger</p> <p>Reminder: Don't code on master branch</p>"},{"location":"git/#the-minimal-workflow-can-be-used-when-coding-alone","title":"The minimal workflow, can be used when coding alone","text":"<pre><code>---\ntitle: Minimal workflow\n---\n%%{init: { 'logLevel': 'debug', 'theme': 'base' } }%%\ngitGraph\n   commit\n   commit\n   branch develop\n   checkout develop\n   commit\n   commit\n   checkout main\n   merge develop\n   commit\ncommit</code></pre>"},{"location":"git/#a-basic-feature-branch-workflow","title":"A basic feature branch workflow","text":"<pre><code>---\ntitle: Basic feature branch workflow\n---\n    %%{init: { 'logLevel': 'debug', 'theme': 'base' } }%%\n    gitGraph\n       commit id: \"1\"\n       commit id: \"2\"\n       branch feature_1\n       checkout feature_1\n       commit id: \"3\"\n       checkout main\n       commit id: \"4\"\n       checkout main\n       branch feature_2\n       checkout feature_2\n       commit id: \"5\"\n       checkout main\n       commit id: \"6\"\n       checkout feature_1\n       commit id: \"7\"\n       checkout main\n       merge feature_1 id: \"customID\" tag: \"customTag\" type: REVERSE\n       checkout feature_2\n       commit id: \"8\"\n       checkout main\n       commit id: \"9\"</code></pre>"},{"location":"git/#best-practices","title":"Best practices","text":""},{"location":"git/#make-single-purpose-small-commits","title":"Make single-purpose &amp; small commits","text":"<p>By creating small commits, it helps everyone in a team to understand what have been done. Also, it's easier to revert a small change in case of bug.</p>"},{"location":"git/#share-only-what-is-necessary-add-gitignore-to-you-repository","title":"Share only what is necessary, add .gitignore to you repository","text":"<p>Info</p> <p><code>.gitignore</code> list all the files and folder that must not be tracked</p> <p>Example</p>"},{"location":"git/#ginignore-simple-example","title":"<code>.ginignore</code> simple example :","text":"<pre><code>__pycache__/\nvenv/\ndata/\ndownload/\nlog.txt\nany_file_you_want_to_exclude.any_extension\n</code></pre>"},{"location":"git/#commit-often-branch-frequently","title":"Commit often &amp; branch frequently","text":"<p>Prefer short-term branch, this will improve the traceability and highly simplify the code review process. Try to not include large number of change in the same branch, and avoid unrelated changes.</p> <p>Tip</p> <p>It's better to commit something un-perfect than nothing  </p>"},{"location":"git/#write-detailed-commit-message-but-short","title":"Write detailed commit message (but short !)","text":"<p>When reading a commit message, anyone should be able to understand what have been done. In general, try to explain what changed from previous code and why.</p> <p>Some good examples: </p> <ul> <li>Change number of epochs from 20 to 40</li> <li>Filter samples that contains null values</li> <li>Change unit from miles to kilometers in compute_distance method</li> </ul> <p>Some bad examples:</p> <ul> <li>Update file1, file2</li> <li>A modification</li> </ul>"},{"location":"python_package_management/","title":"Python package management","text":""},{"location":"python_package_management/#introduction","title":"Introduction","text":"<p>Pip, Anaconda, and Poetry are all package managers for Python, but they have different features and use cases.</p> <p>Info</p> <p>For local development, you can choose between pip &amp; conda depending on your preferences.  In a company, you'll use the one used by the other developers</p>"},{"location":"python_package_management/#pip","title":"Pip","text":"<p>Pip is the default package manager for Python and is used to install and manage Python packages from the Python Package Index (PyPI). Pip is included with Python, and you can use it to install packages globally or in a virtual environment. Pip is simple to use and is suitable for most Python projects.</p>"},{"location":"python_package_management/#anaconda","title":"Anaconda","text":"<p>Anaconda is a distribution of Python that includes many scientific computing packages and tools, such as NumPy, SciPy, and Jupyter notebooks. Anaconda comes with its package manager called conda, which is similar to pip but also includes features for managing non-Python packages and managing virtual environments. Anaconda is particularly useful for data science and machine learning projects because it includes many packages commonly used in those fields.</p>"},{"location":"python_package_management/#poetry","title":"Poetry","text":"<p>Poetry is a newer package manager for Python that aims to simplify dependency management and package distribution. Poetry includes features for managing virtual environments, specifying dependencies and versions, and packaging your project as a distributable package. Poetry is designed to work well with modern Python projects that use tools like Pytest and Black.</p> <p>In summary, if you're working on a simple Python project, pip should suffice. If you're working on a data science or machine learning project, Anaconda might be a better choice. If you're working on a modern Python project that requires advanced dependency management and package distribution, Poetry is worth considering. Ultimately, the choice of package manager depends on your project's requirements and your personal preference.</p>"},{"location":"python_package_management/#commands-recap","title":"Commands recap","text":""},{"location":"python_package_management/#virtual-environment-creation","title":"Virtual environment creation","text":"<p>Warning</p> <ul> <li>You must use virtual env to manage your python projects, basically it will allow you : </li> <li>to prevent version conflicts</li> <li>to create reproducible and easy to install project</li> </ul> <p>There are many way to create virtual env depending on your setup, one you can use : <code>python3 -m venv env_name</code></p>"},{"location":"python_package_management/#pip-usage","title":"Pip usage","text":"<ul> <li>Activate a virtual environment : <code>conda activate env_name</code></li> <li>Install a package : <code>pip install package_name</code></li> <li>Install packages from requirements file : <code>pip install -r requirements.txt</code></li> </ul>"},{"location":"python_package_management/#conda-usage","title":"Conda usage","text":"<ul> <li>Activate a virtual environment : <code>conda activate env_name</code></li> <li>Install a package : <code>conda install package_name</code></li> <li>Install packages from requirements file : <code>conda install --file requirements.txt</code></li> </ul>"},{"location":"practice/airflow/","title":"Airflow in practice","text":""},{"location":"practice/airflow/#installation","title":"Installation","text":"<p>We've seen many way of deploying airflow, for practice, the most straightforward way is to install it via pip and run it in standalone mode.</p> <p>Here a quick tutorial on how to install &amp; run it : airflow documentation</p>"},{"location":"practice/airflow/#create-an-admin-user-to-connect-to-the-ui","title":"Create an admin user to connect to the ui","text":"<p>If you not run all the commands separately, you'll probably need to create a user to connect to the UI</p> <pre><code>airflow users create \\\n--username antoine \\\n--firstname admin \\\n--lastname admin \\\n--role Admin \\\n--password admin \\\n--email admin@example.org\n</code></pre>"},{"location":"practice/airflow/#set-dags-folder","title":"Set dags folder","text":"<p>Tip</p> <p>This is not mandatory, but you can modify the default path of the dag folder to a more convenient place. This is the folder from where airflow will grab the dags you've developed</p> <p>In <code>airflow.cfg</code> file, update <code>dags_folder</code> path to the folder you want on your machine. By default, it's set to your HOME directory.</p>"},{"location":"practice/airflow/#test-to-access-the-ui","title":"Test to access the UI","text":"<p>By default airflow run on localhost:8080. Normally you should have some examples dags that come with the installation if you haven't updated the config file.</p>"},{"location":"practice/airflow/#exercice","title":"Exercice","text":"<p>The aim of the exercise is to create a more realistic dag to retrieve some data from an API, perform some transformation, and store it in a database.</p> <p>Requirements : For this exercise, you'll need : </p> <ul> <li>airflow</li> <li>a database : postgres (feel free to try another)</li> <li>metabase</li> </ul>"},{"location":"practice/airflow/#your-first-dag","title":"Your first dag","text":"<p>Create a hello world dag that run every minute and just print <code>hello world</code>, just to ensure that all is working  as expected.</p>"},{"location":"practice/airflow/#a-real-dag","title":"A real dag","text":"<p>For this exercise, we will use the velib API,</p> <p>Your dag must include (at least the following task):</p> <ul> <li>A task to retrieve the data from the API</li> <li>A task to quickly transform the data (process the data to make it fit the target system)</li> <li>A task to store the result in your a table of your database</li> <li>A task to run some aggregation on the table previously feeded with new data.</li> </ul>"},{"location":"practice/airflow/#1-data-acquisition-task","title":"1 - Data acquisition task","text":"<p>Use the following endpoint : https://velib-metropole-opendata.smoove.pro/opendata/Velib_Metropole/station_status.json to extract the last updated data from the velib api.</p>"},{"location":"practice/airflow/#2-transform-the-data","title":"2 - Transform the data","text":"<p>In this task, you need the json data to be in a flattened format. For each station we want to have the following  information : </p> <ul> <li>station_id</li> <li>number_of_available_docks</li> <li>number_of_electric_bike</li> <li>number_of_mechanical_bike</li> <li>the timestamp at which data has been updated on the server side</li> </ul> <p>Tip</p> <p>You can use the method of your choice, you can do it using pure python, or using some libraries</p>"},{"location":"practice/airflow/#3-store-the-data","title":"3 - Store the data","text":"<p>Warning</p> <p>Before developing the task, you'll need to create a connection to your postgres database</p> <p>Create a task that takes the previously transformed data an insert in a table called <code>velib_station_status</code>. You can manage the table creation from your dag (using <code>CREATE TABLE IF NOT EXISTS</code>, (before the insert statement), or do it using the pg cli.</p> <p>Here is the expected schema of the table : </p> Column Name Description Type id ID of the record - primary key INT station_id ID of the station INT number_of_available_docks Number of available docks at the station INT number_of_electric_bike Number of available electric bikes at the station INT number_of_mechanical_bike Number of available mechanical bikes at the station INT timestamp_data_updated_on_server Timestamp at which data has been updated on the server side TIMESTAMP <p>Here is an example form the airflow documentation</p>"},{"location":"practice/airflow/#4-aggregate-the-data","title":"4 - Aggregate the data","text":"<p>Create a last task which execute a postgres request to count : </p> <ul> <li>the total number of free docks</li> <li>the total number of available mechanical bike</li> <li>the total number of available electric bike</li> </ul> <p>The result must be store in another table : <code>stations_aggregates</code> with the following columns :  Here is the expected schema of the table :</p> Column Name Description Type id ID of the record - Primary key INT number_free_docks Number of free docks at the station INT number_available_e_bikes Number of available electric bikes at the station INT number_available_mechanical_bikes Number of available mechanical bikes at the station INT created_at Timestamp of the computation TIMESTAMP <p>Tip</p>"},{"location":"practice/airflow/#5-plug-metabase-on-your-database","title":"5 - Plug metabase on your database","text":""},{"location":"practice/airflow/#bonus","title":"Bonus","text":"<p>Create a slack integration and add an on_failure callback to send an alert when your dag fail</p>"},{"location":"practice/data_visualization/","title":"Data visualization with Plotly Dash","text":""},{"location":"practice/data_visualization/#tp","title":"TP","text":"<p>The goal of the exercise is to :</p> <ul> <li>Create a web-based dashboard using plotly dash</li> <li>Make it ready for distribution with docker</li> <li>Manage the code versioning with git (in a good manner)</li> </ul> <p>Grading system</p> <p>Your grade will depends on : </p> <ul> <li>The number of steps you've realized. </li> <li>The quality of your code.</li> <li>How you've used git to version your code.</li> <li>Your ability to use work with data to find some great insights. </li> <li>Any additional stuff you've implemented to make your dashboard better (style, layout, filtering).</li> </ul> <p>In order to practice git, I advise you to follow a basic <code>feature-branch</code> workflow :</p> <ul> <li>Don't code directly on your main branch</li> <li>Create a branch foreach step of the exercise (there are 6 step), merge it into the main and start the next step.</li> </ul>"},{"location":"practice/data_visualization/#how-to-submit-your-work","title":"How to submit your work","text":"<p>Danger</p> <p>You have to submit it before 2023-04-23 23:59</p> <p>Your work will be submitted as a Github repository (if you choose to make it private, you have to share it with me).</p> <p>It should have a similar structure to : </p> <pre><code>your_project/\n\u251c\u2500 app.py\n\u251c\u2500 requirements.txt\n\u2514\u2500 Dockerfile\n</code></pre> <p>Feel free to add a README.md file to describe your project, explain how to build the docker image, how to run the  container etc...</p>"},{"location":"practice/data_visualization/#input-data","title":"Input data","text":"<p>Tip</p> <p>Have a look to the data before to deep dive in the exercise</p> <p>For this exercise, you'll use two input dataset (in csv format), you can download them as follows:</p> <ul> <li> <p>One from ratp This dataset represents all the stations managed by RATP in \"Ile de France\" (with each transport mode : subway, train etc...) and  their number of travelers for the year 2021. You also have the city they belong to, and the district.</p> </li> <li> <p></p> </li> <li> <p>One from ile-de-france-mobilit\u00e9s  This dataset contains all the positions of the stations, this time not only for RATP, but also SNCF etc...</p> </li> </ul>"},{"location":"practice/data_visualization/#1-create-some-graph-for-ratp-dataset","title":"1 - Create some graph for RATP dataset","text":"<p>For this question, you need to use the first dataset (the one from RATP). </p>"},{"location":"practice/data_visualization/#objective","title":"Objective","text":"<ul> <li>Create a bar chart that represents the <code>TOP 10</code> stations with the biggest traffic </li> <li>Create a Pie chart that represents trafic per cities (to make it clear, you can take only the TOP 5)</li> <li>Organize those two chart on the same row (they have to be side by side)</li> </ul>"},{"location":"practice/data_visualization/#2-create-some-graph-for-idf-dataset","title":"2 - Create some graph for IDF dataset","text":"<p>For this question, you need to use the first dataset (the one from IDF).</p>"},{"location":"practice/data_visualization/#objective_1","title":"Objective","text":"<ul> <li>Create a bar chart that represents the number of stations per <code>exploitant</code></li> <li>Create a chart that represents the number of stations per <code>ligne</code></li> </ul>"},{"location":"practice/data_visualization/#3-add-some-global-filters","title":"3 - Add some global filters","text":"<p>Add some global filter to your dashboard, use some dropdown selection filter.</p> <p>Tip</p> <p>Feel free to add any filter, it will help you being more familiar with the concept of <code>callback</code></p>"},{"location":"practice/data_visualization/#objective_2","title":"Objective","text":"<ul> <li>One filter for <code>r\u00e9seau</code> (field from the RATP dataset)</li> <li>One filter for <code>exploitant</code> (field from the IDF dataset)</li> <li></li> </ul>"},{"location":"practice/data_visualization/#4-create-an-interactive-map","title":"4 - Create an interactive map","text":"<p>Info</p> <p>For this step, you need to user the second dataset which represents all the subway station in paris, with some  geographical informations</p> <p>In the second dataset, you have many ways to retrieve latitude and longitude, however, you don't have explicit columns <code>lat</code> &amp; <code>lon</code> (or anything similar). One way to to it is to split the <code>geo_point</code> column :</p> <pre><code>df[['lat', 'lng']] = df['geo_point'].str.split(',', expand=True)\ndf['lat'] = df['lat'].str.strip().astype(float)\ndf['lng'] = df['lng'].str.strip().astype(float)\n</code></pre>"},{"location":"practice/data_visualization/#objective_3","title":"Objective","text":"<p>Add a Map to your dashboard to visualize the position of the stations.</p>"},{"location":"practice/data_visualization/#5-containerize-your-plotly-dash-application","title":"5 - Containerize your plotly dash application","text":"<p>Tip</p> <p>Have a look to the docker section, there is an example with a flask application, it's different but the logic is very similar</p>"},{"location":"practice/data_visualization/#objective_4","title":"Objective","text":"<ul> <li>Create a <code>Dockerfile</code> in which you define how the docker image must be build in order to run your application</li> <li>Build your docker image : <code>docker build -t your_image .</code></li> <li>Run a container : be careful you need to expose port <code>8050</code> which is the port used by plotly dash</li> </ul> <p>In order to make you application accessible from outside the container, you need to set the host to <code>0.0.0.0</code>, you can do it by update the run server :  <pre><code>app.run_server(host='0.0.0.0', port=8050, debug=True)\n</code></pre></p>"},{"location":"practice/data_visualization/#bonus-add-some-style-to-your-dashboard","title":"Bonus - Add some style to your dashboard","text":""},{"location":"practice/docker/","title":"Docker","text":""},{"location":"practice/docker/#docker-in-practice","title":"Docker in practice","text":"<p>Let's setup a local environment with the following components : </p> <ul> <li>A database : postgresql</li> <li>An database administration platform : pgadmin</li> <li>A Business intelligence app : metabase</li> </ul> <p>Danger</p> <p>This is a local setup, in a real life project, you'll not host your database in a docker container Also, metabase use a database to store its metadata, by default it comes with an sqllite, which must be replaced by another external database like postgres or mysql</p>"},{"location":"practice/docker/#docker-a-simple-way-to-setup-your-local-environment","title":"Docker, a simple way to setup your local environment","text":"<p>You can deploy any application using a very short command... with docker !</p>"},{"location":"practice/docker/#deploy-a-database","title":"Deploy a database","text":"<pre><code>docker run -d \\\n--name postgres \\\n-p 5432:5432 \\\n-e POSTGRES_PASSWORD=password \\\n-v postgres:/var/lib/postgresql/data \\\npostgres:15.2\n</code></pre> <p>Let's decompose this command : </p> Argument Description <code>docker run</code> Command to start a new container <code>-d</code> Runs the container in the background (detached mode) <code>--name postgres</code> Assigns the name <code>postgres</code> to the new container <code>-p 5432:5432</code> Maps port 5432 on the host machine to port 5432 in the container <code>-e POSTGRES_PASSWORD=password</code> Sets an environment variable <code>POSTGRES_PASSWORD</code> with the value <code>password</code> inside the container <code>-v postgres:/var/lib/postgresql/data</code> Mounts a Docker volume named <code>postgres</code> to the container directory <code>/var/lib/postgresql/data</code> <code>postgres:15.2</code> Specifies the name and tag of the PostgreSQL Docker image to use for the new container <p>Now you've a running postgres instance which is listening on port 5432, you can connect to it using <code>psql</code> or any client.</p> <p>You can check it by running <code>docker ps</code> or directly in docker desktop if you prefer the UI.</p> <p>You will have an output like this :  <pre><code>CONTAINER ID   IMAGE           COMMAND                  CREATED         STATUS         PORTS                    NAMES\n3ca9e24601b5   postgres:15.2   \"docker-entrypoint.s\u2026\"   5 seconds ago   Up 4 seconds   0.0.0.0:5432-&gt;5432/tcp   postgres\n</code></pre></p> <p>Tip</p> <p>It's better to use the cli (the commands) instead of the UI, it will help you to become more familiar with docker</p> <p></p>"},{"location":"practice/docker/#deploy-pgadmin","title":"Deploy pgadmin","text":"<p>For the exercise, let's say you want setup a web client to monitor and administrate your database. Let's deploy pg admin !</p> <pre><code>docker run \\\n-p 5050:80 \\\n-e \"PGADMIN_DEFAULT_EMAIL=email@example.com\" \\\n-e \"PGADMIN_DEFAULT_PASSWORD=password\" \\\n-d dpage/pgadmin4:latest\n</code></pre> <p>To be sure you understand... let's describe the command again</p> Argument Description <code>docker run</code> Command to start a new container <code>-p 5050:80</code> Maps port 80 inside the container to port 5050 on the host machine <code>-e \"PGADMIN_DEFAULT_EMAIL=email@example.com\"</code> Sets an environment variable <code>PGADMIN_DEFAULT_EMAIL</code> with the value <code>email@example.com</code> inside the container <code>-e \"PGADMIN_DEFAULT_PASSWORD=password\"</code> Sets an environment variable <code>PGADMIN_DEFAULT_PASSWORD</code> with the value <code>password</code> inside the container <code>-d</code> Runs the container in the background (detached mode) <code>dpage/pgadmin4:latest</code> Specifies the name and tag of the pgAdmin4 Docker image to use for the new container <p>Info</p> <p>You'll notice that this time, the version is set using :latest, this is a tag that refers to the most recent version of a Docker image. It's better to specify a version like dpage/pgadmin4:4.32 this would ensure that you always use the same version of the image, regardless of whether a new \"latest\" version is released.</p> <p> Now, if you go to localhost:5050 you'll be able to access the pg admin interface. </p> <p></p> <p>If you perform a <code>docker ps</code> you'll see two containers, one for postgres and one for pgadmin</p> <pre><code>CONTAINER ID   IMAGE               COMMAND                  CREATED         STATUS         PORTS                                       NAMES\n3ca9e24601b5   postgres:15.2      \"docker-entrypoint.s\u2026\"   5 minutes ago   Up 5 minutes   0.0.0.0:5432-&gt;5432/tcp, :::5432-&gt;5432/tcp   postgres\na08f7a258dbf   dpage/pgadmin4:4.23 \"/entrypoint.sh\"         5 minutes ago   Up 5 minutes   0.0.0.0:5050-&gt;80/tcp, :::5050-&gt;80/tcp       pgadmin\n</code></pre> <p></p>"},{"location":"practice/docker/#deploy-a-bi-platform-metabase","title":"Deploy a BI platform, metabase","text":"<p>There are many BI platforms, however only few are open-source. In addition, metabase can be deployed in seconds...  with docker.  <pre><code>docker run -d -p 3000:3000 --name metabase metabase/metabase\n</code></pre>  If you perform a docker ps you'll see 3 containers :  <pre><code>CONTAINER ID   IMAGE               COMMAND                  CREATED         STATUS         PORTS                                       NAMES\n3ca9e24601b5   postgres:15.2      \"docker-entrypoint.s\u2026\"   5 minutes ago   Up 5 minutes   0.0.0.0:5432-&gt;5432/tcp, :::5432-&gt;5432/tcp   postgres\na08f7a258dbf   dpage/pgadmin4:4.23 \"/entrypoint.sh\"         5 minutes ago   Up 5 minutes   0.0.0.0:5050-&gt;80/tcp, :::5050-&gt;80/tcp       pgadmin\ncc8a7f165c43   metabase/metabase:0.45 \"/app/run_metabase.s\u2026\"  5 seconds ago    Up 4 seconds    0.0.0.0:3000-&gt;3000/tcp                     metabase\n</code></pre></p> <p> Metabase is now accessible on localhost:3000. You'll have to wait a bit of time to be able to access it and have  a page like this:  </p> <p></p> <p></p>"},{"location":"practice/docker/#all-together-docker-compose","title":"All together, docker-compose","text":"<p>We've seen how to run single container application, using separated commands, but this is not very convenient... As previously mentioned, there is a way to manage multi-container applications : <code>docker-compose</code></p> <p>Info</p> <p>You will not have to learn commands for both <code>docker</code> and <code>docker-compose</code>, basically you'll find the  same commands. Ex : <code>docker ps</code> become <code>docker-compose ps</code></p> <p>In order to use docker-compose, you need to write a <code>docker-compose.yml</code> file that describe each container you want to  deploy.</p> <p>Here is an example of a file describing our 3 previously deployed application : </p> <p></p> <pre><code>version: \"3\"\nservices:\npostgres:\nimage: postgres:14.2-alpine\nrestart: always\nenvironment:\nPOSTGRES_PASSWORD: postgres\nPOSTGRES_USER: postgres\nPOSTGRES_HOST: \"0.0.0.0\"\nvolumes:\n- postgres:/var/lib/postgresql/data\npgadmin:\nimage: dpage/pgadmin4:4.23\nenvironment:\nPGADMIN_DEFAULT_EMAIL: admin@pgadmin.com\nPGADMIN_DEFAULT_PASSWORD: password\nPGADMIN_LISTEN_PORT: 80\nports:\n- 8080:80\nvolumes:\n- pgadmin:/var/lib/pgadmin\ndepends_on:\n- postgres\nmetabase:\nimage: metabase/metabase\nports:\n- \"3000:3000\"\nrestart: always\nvolumes:\npostgres:\npgadmin:\n</code></pre> <p>Inside the folder where you <code>docker-compose.yml</code> file is located, simply run <code>docker-compose up -d</code>.</p> <p>Info</p> <ul> <li>The version \"3\" at the beginning of the file indicates that this is a Docker Compose file written in version 3 syntax.</li> <li>The file defines three services: postgres, pgadmin, and metabase.</li> <li>The postgres service runs the official PostgreSQL 14.2-alpine image, which is a lightweight version of the PostgreSQL database running on the Alpine Linux distribution. It sets environment variables for the database user and password, and also specifies a volume to store the database data.</li> <li>The pgadmin service runs the dpage/pgadmin4:4.23 image, which is a web-based administration tool for PostgreSQL. It sets environment variables for the default email and password, and exposes port 8080 on the host machine to port 80 on the container. It also specifies a volume to store pgAdmin data, and depends on the postgres service to be running.</li> <li>The metabase service runs the metabase/metabase image, which is a business intelligence and analytics tool. It exposes port 3000 on the host machine to port 3000 on the container, and sets the restart policy to always.</li> <li>The volumes section defines two named volumes, postgres and pgadmin, which are used by the postgres and pgadmin services respectively to store data.</li> </ul>"}]}